function ex4()
%% Machine Learning Online Class - Exercise 4 Neural Network Learning

%  Instructions
%  ------------
%
%  Этот файл содержит код, который поможет вам начать работу слинейных упражнений.
% Вам нужно будет выполнить следующие функции в этом упражнении:
%     sigmoidGradient.m
%     randInitializeWeights.m
%     nnCostFunction.m
%
% Для этого упражнения вам не нужно изменять код в этом файле,
% или любые другие файлы, кроме упомянутых выше.
%

%% Initialization
clear ; close all; clc

%% Настройте параметры, которые вы будете использовать для этого упражнения
input_layer_size  = 400;  % 20x20 Input Images of Digits
hidden_layer_size = 25;   % 25 hidden units
num_labels = 10;          % 10 labels, from 1 to 10
% (обратите внимание, что мы присвоили «0» метке 10)


%% =========== Part 1: Loading and Visualizing Data =============
% Мы начинаем упражнение с первой загрузки и визуализации набора данных.
% Вы будете работать с набором данных, который содержит рукописные цифры.
%

% Load Training Data
fprintf('Loading and Visualizing Data ...\n')

% Загрузим X,y
load('ex4data1.mat');
m = size(X, 1);

% Случайно выберите 100 точек данных для отображения
sel = randperm(size(X, 1));
sel = sel(1:100);

displayData(X(sel, :));

fprintf('Программа приостановлена. Нажмите Enter, чтобы продолжить.\n');
fprintf('----------------------------------------------------------\n');
pause;


%% ================ Part 2: Loading Parameters ================
% В этой части упражнения мы загружаем некоторые предварительно инициализированные
% параметров нейронной сети.

    fprintf('\nЗагрузка сохраненных параметров нейронной сети ...\n')

% Загрузите веса в переменные Theta1 и Theta2
load('ex4weights.mat');

% Развернуть параметры
nn_params = [Theta1(:) ; Theta2(:)];


%% ================ Part 3: Compute Cost (Feedforward) ================
% К нейронной сети, вы должны сначала начать с реализации
% прямой части нейронной сети, которая возвращает только стоимость.
% Вы должнs завершить код в nnCostFunction.m, чтобы вернуть стоимость.
% После реализации прямой связи для расчета стоимости, вы можете убедиться, что
% Ваша реализация верна, проверяя, что вы получаете ту же стоимость
% как мы для фиксированных параметров отладки.
%
% Мы предлагаем реализовать прямую стоимость сначала *без* регуляризации
% ,чтобы вам было легче отлаживать. 
% Позже, в части 4, вы получит для реализации регуляризационной стоимости/затрат.
%
fprintf('\nПрямая связь с использованием нейронной сети ...\n')

% Параметр регуляризации веса (здесь мы устанавливаем это значение 0)
    lambda = 0;

J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...
num_labels, X, y, lambda);

fprintf(['Стоимость по параметрам (загружается из ex4weights): %f '...
    '\n(это значение должно быть около 0.287629)\n'], J);

fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
fprintf('----------------------------------------------------------\n');
pause;


%% =============== Part 4: Implement Regularization ===============
% Как только ваша реализация функции стоимости будет правильной, вы должны сейчас
% продолжают внедрять регуляризацию со стоимостью.
%

fprintf('\nПроверка функции затрат (с  Регуляризацией) ... \n')

% Параметр регуляризации веса (здесь мы устанавливаем в 1).
    lambda = 1;

J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...
num_labels, X, y, lambda);

fprintf(['Стоимость по параметрам (загружается из ex4weights): %f '...
    '\n(это значение должно быть около 0.383770)\n'], J);

fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
fprintf('----------------------------------------------------------\n');
pause;


%% ================ Part 5: Sigmoid Gradient  ================
% Прежде чем вы начнете внедрять нейронную сеть, вы должны сначала
% реализовать градиент для сигмовидной функции.
% Вы должны завершить код в файле sigmoidGradient.m.

fprintf('\nОценка сигмовидного градиента...\n')

g = sigmoidGradient([-1 -0.5 0 0.5 1]);
fprintf('Сигмовидный градиент оценивается при [-1 -0.5 0 0.5 1]:\n  ');
fprintf('%f ', g);
fprintf('\n\n');

fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
fprintf('----------------------------------------------------------\n');
pause;


%% ================ Part 6: Initializing Pameters ================
% В этой части упражнения вы начнете реализовывать два
% уровня нейросети, которая классифицирует цифры. 
% Вы начнете с реализации функции для инициализации весов нейронной сети
% (randInitializeWeights.m)

fprintf('\nИнициализация параметров нейронной сети ...\n')

initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);
initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);

% Развернуть параметры
initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];


%% =============== Part 7: Implement Backpropagation ===============
% Как только ваши расходы совпадают с нашими, вы должны приступить к реализации
% алгоритма обратного распространения для нейронной сети. 
% Вы должны добавить код, который вы написали в nnCostFunction.m 
% для возврата частичного производных параметров.
%
fprintf('\nПроверка обратного распространения... \n');

%  Проверьте градиенты, запустив checkNNGradients
checkNNGradients;

fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
fprintf('----------------------------------------------------------\n');
pause;



%% =============== Part 8: Implement Regularization ===============
% Как только ваша реализация обратного распространения верна, вы должны сейчас
% продолжают внедрять регуляризацию со стоимостью и градиентом.
%

  fprintf('\nПроверка обратного распространения (с / Регуляризация) ... \n')

  %  Проверьте градиенты, запустив checkNNGradients
  lambda = 3;
  checkNNGradients(lambda);

  % Также выведите значения отладки costFunction
  debug_J  = nnCostFunction(nn_params, input_layer_size, ...
  hidden_layer_size, num_labels, X, y, lambda);

  fprintf(['\n\nСтоимость при (фиксированных) параметрах отладки (w/ lambda = %f): %f ' ...
      '\n(for lambda = 3, это значение должно быть около 0.576051)\n\n'], lambda, debug_J);

  fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
  fprintf('----------------------------------------------------------\n');
  pause;


%% =================== Part 8: Training NN ===================
% Теперь вы реализовали весь код, необходимый для обучения нейронной сети. 
% Чтобы обучить вашу нейронную сеть, теперь мы будем использовать «fmincg», который
% - это функция, которая работает аналогично «fminunc».
% Напомним, что эти продвинутых оптимизаторов могут эффективно тренировать наши
% функции затрат, пока мы предоставляем им вычисления градиента.
%
#{
  После того, как вы успешно внедрили функцию стоимости нейронной сети и вычисления 
  градиента, запустите код ниже, чтобы использовать fmincg для изучения хорошего набора параметров. 
  После завершения обучения код будет
  Сообщите о точности обучения вашего классификатора, рассчитав процент правильных примеров. 
  Если ваш
  реализация правильная, вы должны увидеть заявленную точность обучения около 95,3% 
  (это может варьироваться примерно
  1% из-за случайной инициализации).
  Можно получить более высокую точность обучения, обучая нейронную сеть
  для большего количества итераций.
#}
  fprintf('\nОбучение нейронной сети... \n')

  % Вам также следует попробовать разные значения лямбды
  lambda = 1;
  
  MaxIter = 50;
  % После того, как вы завершили назначение, измените MaxIter на большее
  % значения, чтобы увидеть, как помогает тренировка.
  options = optimset('MaxIter', MaxIter);

 

  % Создайте "short hand"«короткую руку» для минимизации функции стоимости.
  costFunction = @(p) nnCostFunction(p, ...
  input_layer_size, ...
  hidden_layer_size, ...
  num_labels, X, y, lambda);

  % Теперь, costFunction - это функция, которая принимает только один аргумент
  % (параметров нейронной сети)
  [nn_params, cost] = fmincg(costFunction, initial_nn_params, options);

  % Получить Theta1 и Theta2 обратно из nn_params
  Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
  hidden_layer_size, (input_layer_size + 1));

  Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
  num_labels, (hidden_layer_size + 1));

  pred = predict(Theta1, Theta2, X);
  fprintf('\nТочность тренировочного набора: %f\n', mean(double(pred == y)) * 100);

  fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
  fprintf('----------------------------------------------------------\n');
  pause;


%% ================= Part 9: Visualize Weights =================
% Теперь вы можете «визуализировать» то, что нейронная сеть изучает
% отображения скрытых единиц, чтобы увидеть, какие функции они захватывают в
%  данные.

    fprintf('\nVisualizing Neural Network... \n')
    % нужно 400 единиц вектора преобразовать в 20х20
    displayData(Theta1(:, 2:end));

    fprintf('\nПрограмма приостановлена. Нажмите ввод, чтобы продолжить.\n');
    fprintf('----------------------------------------------------------\n');
    pause;

%% ================= Part 10: Implement Predict =================
% После обучения нейронной сети, мы хотели бы использовать ее для прогнозирования
% этикетки. Теперь вы будете реализовывать функцию «прогнозирования predict», 
% чтобы использовать нейронной сети для прогнозирования меток обучающего набора. 
% Это позволяет вычисляеть точность тренировочного набора.

    pred = predict(Theta1, Theta2, X);

fprintf('\nТочность тренировочного набора: %f\n', mean(double(pred == y)) * 100);


 
 
    % Чтобы дать вам представление о выходе сети, вы также можете запустить
    % через примеры по одному, чтобы увидеть, что он предсказывает.

    % Случайно переставлять примеры
    rp = randperm(m);

    for i = 1:m
      % Display
      fprintf('\nОтображение примера изображения\n');
      displayData(X(rp(i), :));

      pred = predict(Theta1, Theta2, X(rp(i),:));
      if pred==10,pred=0;endif;
      fprintf('\nПредсказание нейронной сети: %d (digit %d)\n', pred, mod(pred, 10));

      % Pause with quit option
      s = input('Приостановлено - нажмите Enter для продолжения, `q` для выхода:','s');
      if s == 'q'
       break
      endif
    
     endfor
     
     
endfunction